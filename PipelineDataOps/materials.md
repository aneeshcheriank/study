## Data Pipelines
- AWS Lambda + Stepfunctions
- Lambad function 
    - trigger
    - function
    - 
- boiler plate code  (Lambda function)
    ```
    import json

    def lambda_handler(event, context)
        # TODO implement
        return{
            'statusCode': 200,
            'body': json.dumps('Hello from Lambda!')
        }
    ```

## Query Databricks Pipeline
- Azure databricks

## Data ingestion pipeline (AWS)
![alt text](./images/image.png)

## AWS Stepfunctions
- orchestrating jobs

## Transforming Data in Transit
- ![alt text](./images/image_1.png)

AWS Batch

## Serverless AI Data Engineering Pipeline
![alt text](./images/image_2.png)

## Additional readings
|Title|Type|Length|Brief Description|
|---|---|---|---|
|[Developing with Messaging Services](https://docs.google.com/presentation/d/1tvHVXe_cWSGeOKB84pCCfMsm4_OlLOj_/edit#slide=id.p1)|Reaing|15-30|This reading takes the student through an official AWS tutorial on using AWS messaging services.|
|[Databricks Utilities](https://docs.databricks.com/dev-tools/databricks-utils.html#file-system-utility-dbutilsfs)|Documentation|5-10|Databricks Utilities (dbutils) provide a powerful set of tools to help you perform various tasks efficiently within notebooks. These utilities can be used to interact with object storage, chain and parameterize notebooks, and manage secrets. Keep in mind that dbutils are not supported outside of notebooks and should not be used within executors, as they may produce unexpected results. For more information on the limitations of dbutils and potential alternatives, refer to the Limitations section.<br><br>Dbutils are available in Python, R, and Scala notebooks, and you can access the following utilities: credentials, data, fs, jobs, library, notebook, secrets, and widgets, along with the Utilities API library. To learn how to list utilities, list commands, and display command help, refer to the respective sections in the Databricks documentation. This resource can serve as a helpful reference when working with dbutils in your Coursera course, offering valuable insights into their usage and capabilities.|
|[Serversless Reference Architecture: Real-time File Processing](https://github.com/aws-samples/lambda-refarch-fileprocessing)|Reading|15-30|The Real-time File Processing reference architecture is a versatile, event-driven, parallel data processing system utilizing AWS Lambda, designed for workloads requiring multiple data derivatives of an object. In the example provided, interview notes in Markdown format are delivered to S3. S3 Events trigger multiple processing flows, including one that converts and stores Markdown files as HTML, and another that detects and stores sentiment information. This architecture is ideal for applications that demand real-time file processing and parallel data processing capabilities.|
|[AWS CDK Examples](https://github.com/aws-samples/aws-cdk-examples)|Reading|15-30|The AWS CDK Examples repository offers a collection of example projects for the AWS Cloud Development Kit (CDK), divided into sections for each supported language. These examples demonstrate common service implementations and infrastructure patterns to help you use the CDK effectively when building your own infrastructure. The repository is considered an intermediate learning resource and is best utilized after reviewing the Developer Guide or CDK Workshop. In addition to the examples, there are other learning resources available to assist with your learning and development process. Contributions to this repository in the form of fixes or new examples are highly encouraged, with a CONTRIBUTING guide provided for more information.|