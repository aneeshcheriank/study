{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebcfde3-06e6-4d6c-96d8-2946c81c8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22fc0fc-5b9c-45f0-8b90-0ef4ad875762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aneesh/RAG/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import importlib\n",
    "from typing import List, Optional\n",
    "import chromadb\n",
    "\n",
    "import gemini_auth\n",
    "importlib.reload(gemini_auth)\n",
    "\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, DocArrayInMemorySearch\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "DOC_FOLDER = './doc'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 150\n",
    "VECTOR_DIR = './out/vector_dir'\n",
    "\n",
    "\n",
    "def get_files(doc_folder):\n",
    "    return glob.glob(f'{doc_folder}/*.pdf')\n",
    "\n",
    "def split_docs(files):\n",
    "    docs = []\n",
    "    for file in files:\n",
    "      # load pdf documents\n",
    "      loader = PyPDFLoader(file)\n",
    "      documents = loader.load()\n",
    "      \n",
    "      # split the documents into chunks\n",
    "      text_splitter = RecursiveCharacterTextSplitter(chunk_size = CHUNK_SIZE, chunk_overlap = CHUNK_OVERLAP)\n",
    "      docs.extend(text_splitter.split_documents(documents))\n",
    "      \n",
    "    return docs\n",
    "\n",
    "# need to create a class to convert the output of the embedding class to a list\n",
    "class GeminiEmbeddings(GoogleGenerativeAIEmbeddings):\n",
    "    def embed_documents(self, texts: List[str],\n",
    "                        task_type: Optional[str] = None,\n",
    "                        titles: Optional[List[str]] = None,\n",
    "                        output_dimensionality: Optional[int] = None) -> List[List[float]]:\n",
    "\n",
    "        embeddings = super().embed_documents(texts, task_type, titles, output_dimensionality)\n",
    "        # Convert Repeated type to list type\n",
    "        return list(map(list, embeddings))\n",
    "\n",
    "def add_data_into_vector_db(docs, embedding_function):\n",
    "    vectordb = Chroma.from_documents(\n",
    "      docs, embedding_function, \n",
    "      persist_directory=VECTOR_DIR\n",
    "    )\n",
    "\n",
    "\n",
    "def retirve_chunks(question, embembedding_function):\n",
    "    '''\n",
    "    collect similar chunks from an existing vector db\n",
    "    inputs:\n",
    "        1. db\n",
    "        2. question\n",
    "    output: docs: similarity docs \n",
    "    '''\n",
    "    vectordb = Chroma.from_documents(\n",
    "      persist_directory=VECTOR_DIR, \n",
    "      embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    docs = vectordb.similarity_search(question, k=3)\n",
    "    return docs\n",
    "\n",
    "def retrieve_info(llm, question):\n",
    "    vectordb = Chroma.from_documents(\n",
    "      persist_directory=VECTOR_DIR, \n",
    "      embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm, retriever=vectordb.as_retriever()\n",
    "    )\n",
    "    \n",
    "    result = qa_chain({'query': question})\n",
    "    return result['result']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    files = get_files('./doc')\n",
    "    docs = split_docs(files)\n",
    "    question = 'what is the scope of this document'\n",
    "\n",
    "    embedding_function = GeminiEmbeddings(model=\"models/embedding-001\")\n",
    "    llm = ChatGoogleGenerativeAI(model='gemini-pro')\n",
    "\n",
    "    answer = retrieve_info(llm, question)\n",
    "    print(answer)  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b593e-32ae-4d6a-a6e9-9ab16405bd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
